{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Setup\n",
    "Import your dataflow libraries and initilize the pipeline runner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import google.cloud.dataflow as df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello world\n",
    "Create a transform from an iterable and use the pipe operator to chain transforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.direct_runner.DirectPipelineResult at 0x110f30f90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipeline executing on a direct runner (local, non-cloud).\n",
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "\n",
    "# Create a PCollection with names and write it to a file.\n",
    "(p\n",
    " | df.Create('add names', ['Ann', 'Joe'])\n",
    " | df.Write('save', df.io.TextFileSink('./names')))\n",
    "# Execute the pipeline.\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello world (with Map)\n",
    "The Map transform takes a callable, which will be applied to each element of the input PCollection and must return an element to go into the output PCollection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.direct_runner.DirectPipelineResult at 0x110e643d0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipeline executing on a direct runner (local, non-cloud).\n",
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "\n",
    "# Read file with names, add a greeting for each, and write results.\n",
    "(p\n",
    " | df.Read('load messages', df.io.TextFileSource('./names*'))\n",
    " | df.Map('add greeting', lambda name, msg: '%s %s!' % (msg, name), 'Hello')\n",
    " | df.Write('save', df.io.TextFileSink('./greetings')))\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello world (with FlatMap)\n",
    "A `FlatMap` is like a `Map` except its callable returns a (possibly empty) iterable of elements for the output `PCollection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.direct_runner.DirectPipelineResult at 0x110e83850>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "# Read previous file, add a name to each greeting and write results.\n",
    "(p\n",
    " | df.Read('load messages', df.io.TextFileSource('./names*'))\n",
    " | df.FlatMap('add greetings', lambda name, msgs: ['%s %s!' % (m, name) for m in msgs], ['Hello', 'Hola'])\n",
    " | df.Write('save', df.io.TextFileSink('./greetings')))\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello world (with FlatMap and yield)\n",
    "The callable of a `FlatMap` can be a generator, that is, a function using `yield`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.direct_runner.DirectPipelineResult at 0x110e56990>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "# Add greetings using a FlatMap function using yield.\n",
    "def add_greetings(name, messages):\n",
    "    for m in messages:\n",
    "        yield '%s %s!' % (m, name)\n",
    "\n",
    "(p\n",
    " | df.Read('load names', df.io.TextFileSource('./names*'))\n",
    " | df.FlatMap('greet', add_greetings, ['Hello', 'Hola', 'Hi'])\n",
    " | df.Write('save', df.io.TextFileSink('./greetings')))\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting words\n",
    "This example counts the words in a text and also shows how to read a text file from [Google Cloud Storage](https://cloud.google.com/storage/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.direct_runner.DirectPipelineResult at 0x110f66350>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "\n",
    "(p\n",
    " | df.Read('read', df.io.TextFileSource('gs://dataflow-samples/shakespeare/kinglear.txt'))\n",
    " | df.FlatMap('split', lambda x: re.findall(r'\\w+', x))\n",
    " | df.combiners.Count.PerElement('count words')\n",
    " | df.Write('write', df.io.TextFileSink('./results')))\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting words with GroupByKey\n",
    "Here we use `GroupByKey` to count the words. \n",
    "This is a somewhat forced example of `GroupByKey`; \n",
    "normally one would use the transform `df.combiners.Count.PerElement`, \n",
    "as in the previous example. The example also shows the use of a wild-card in specifying the text file source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.direct_runner.DirectPipelineResult at 0x110f66ed0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "\n",
    "class MyCountTransform(df.PTransform):\n",
    "    def apply(self, pcoll):\n",
    "        return (pcoll\n",
    "        | df.Map('one word', lambda w: (w, 1))\n",
    "        # GroupByKey accepts a PCollection of (w, 1) and\n",
    "        # outputs a PCollection of (w, (1, 1, ...))\n",
    "        | df.GroupByKey('group words')\n",
    "        | df.Map('count words', lambda (word, counts): (word, len(counts))))\n",
    "\n",
    "(p\n",
    "| df.Read('read', df.io.TextFileSource('./names*'))\n",
    "| df.FlatMap('split', lambda x: re.findall(r'\\w+', x))\n",
    "| MyCountTransform()\n",
    "| df.Write('write', df.io.TextFileSink('./results')))\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type hints\n",
    "In some cases, you can improve the efficiency of the data encoding by providing type hints. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.direct_runner.DirectPipelineResult at 0x110f1be90>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud.dataflow.typehints import typehints\n",
    "\n",
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "\n",
    "(p\n",
    " | df.Read('A', df.io.TextFileSource('./names*'))\n",
    " | df.Map('B1', lambda x: (x, 1)).with_output_types(typehints.KV[str, int])\n",
    " | df.GroupByKey('GBK')\n",
    " | df.Write('C', df.io.TextFileSink('./results')))\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigQuery\n",
    "Here is a pipeline that reads input from a BigQuery table and writes the result to a different table. This example calculates the number of tornadoes per month from weather data. To run it you will need to provide an output table that you can write to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.direct_runner.DirectPipelineResult at 0x110d6dc10>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_table = 'clouddataflow-readonly:samples.weather_stations'\n",
    "project = 'djomniture'\n",
    "\n",
    "p = df.Pipeline(argv=['--project', project])\n",
    "(p\n",
    " | df.Read('read', df.io.BigQuerySource(input_table))\n",
    " | df.FlatMap( 'months with tornadoes', lambda row: [(int(row['month']), 1)] if row['tornado'] else [])\n",
    " | df.CombinePerKey('monthly count', sum)\n",
    " | df.Map('format', lambda (k, v): {'month': k, 'tornado_count': v})\n",
    " | df.Write('write', df.io.TextFileSink('./tornadoes')))\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a pipeline that achieves the same functionality, i.e., calculates the number of tornadoes per month, but uses a query to filter out input instead of using the whole table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.direct_runner.DirectPipelineResult at 0x110ee3290>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = 'djomniture'\n",
    "input_query = 'SELECT month, COUNT(month) AS tornado_count ' \\\n",
    "        'FROM [clouddataflow-readonly:samples.weather_stations] ' \\\n",
    "        'WHERE tornado=true GROUP BY month'\n",
    "p = df.Pipeline(argv=['--project', project])\n",
    "(p\n",
    "| df.Read('read', df.io.BigQuerySource(query=input_query))\n",
    "| df.Write('write', df.io.TextFileSink('./tornadoes')))\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combiner Examples\n",
    "A common case for Dataflow combiners is to sum (or max or min) over the values of each key. Such standard Python functions can be used directly as combiner functions. In fact, any function \"reducing\" an iterable to a single value can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.direct_runner.DirectPipelineResult at 0x110ee3210>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "\n",
    "SAMPLE_DATA = [('a', 1), ('b', 10), ('a', 2), ('a', 3), ('b', 20)]\n",
    "\n",
    "(p\n",
    " | df.Create(SAMPLE_DATA)\n",
    " | df.CombinePerKey(sum)\n",
    " | df.Write(df.io.TextFileSink('./results')))\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
